{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index Product Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip pyyaml fastparquet > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by extracting product data from the XML files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of XML files: 256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/workspace/datasets/product_data/products/products_0001_2570_to_430420.xml',\n",
       " '/workspace/datasets/product_data/products/products_0002_430439_to_518210.xml',\n",
       " '/workspace/datasets/product_data/products/products_0003_518229_to_606384.xml',\n",
       " '/workspace/datasets/product_data/products/products_0004_606428_to_722720.xml',\n",
       " '/workspace/datasets/product_data/products/products_0005_722800_to_846222.xml']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "SOURCE_DIR = \"/workspace/datasets/product_data/products\"\n",
    "FILES = glob.glob(f\"{SOURCE_DIR}/*.xml\")\n",
    "print(f\"Number of XML files: {len(FILES)}\")\n",
    "FILES[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us read `mappings.yaml` to get the xpath selectors associated with each field to extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['accessories', 'active', 'artistName', 'bestBuyItemId', 'bestSellingRank', 'categoryLeaf', 'categoryPath', 'categoryPathCount', 'categoryPathIds', 'class', 'classId', 'color', 'condition', 'crossSell', 'customerReviewAverage', 'customerReviewCount', 'department', 'departmentId', 'depth', 'description', 'digital', 'features', 'frequentlyPurchasedWith', 'height', 'homeDelivery', 'image', 'inStoreAvailability', 'inStorePickup', 'longDescription', 'longDescriptionHtml', 'manufacturer', 'modelNumber', 'name', 'onSale', 'onlineAvailability', 'productId', 'quantityLimit', 'regularPrice', 'relatedProducts', 'releaseDate', 'salePrice', 'salesRankLongTerm', 'salesRankMediumTerm', 'salesRankShortTerm', 'shippingCost', 'shippingWeight', 'shortDescription', 'shortDescriptionHtml', 'sku', 'startDate', 'subclass', 'subclassId', 'type', 'url', 'weight', 'width'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "with open(\"/workspace/datasets/mappings.yaml\", \"r\") as handle:\n",
    "    mappings = yaml.safe_load(handle)\n",
    "mappings.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us write a function to extract product records from a XML file and return a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accessories</th>\n",
       "      <th>active</th>\n",
       "      <th>artistName</th>\n",
       "      <th>bestBuyItemId</th>\n",
       "      <th>bestSellingRank</th>\n",
       "      <th>categoryLeaf</th>\n",
       "      <th>categoryPath</th>\n",
       "      <th>categoryPathCount</th>\n",
       "      <th>categoryPathIds</th>\n",
       "      <th>class</th>\n",
       "      <th>...</th>\n",
       "      <th>shortDescription</th>\n",
       "      <th>shortDescriptionHtml</th>\n",
       "      <th>sku</th>\n",
       "      <th>startDate</th>\n",
       "      <th>subclass</th>\n",
       "      <th>subclassId</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>weight</th>\n",
       "      <th>width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>[false]</td>\n",
       "      <td>[Redbone,Leon]</td>\n",
       "      <td>[425502]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[cat02005]</td>\n",
       "      <td>[Best Buy, Movies &amp; Music, Music, Folk]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[cat00000, abcat0600000, cat02001, cat02005]</td>\n",
       "      <td>[COMPACT DISC]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[430439]</td>\n",
       "      <td>[1989-11-24]</td>\n",
       "      <td>[ROCK]</td>\n",
       "      <td>[1001]</td>\n",
       "      <td>[Music]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>[false]</td>\n",
       "      <td>[Shadowfax]</td>\n",
       "      <td>[184387]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[cat02008]</td>\n",
       "      <td>[Best Buy, Movies &amp; Music, Music, New Age]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[cat00000, abcat0600000, cat02001, cat02008]</td>\n",
       "      <td>[COMPACT DISC]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[430448]</td>\n",
       "      <td>[1990-07-10]</td>\n",
       "      <td>[JAZZ-CONTEMPORARY]</td>\n",
       "      <td>[1002]</td>\n",
       "      <td>[Music]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>[true]</td>\n",
       "      <td>[The Pointer Sisters]</td>\n",
       "      <td>[321638]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[cat02011]</td>\n",
       "      <td>[Best Buy, Movies &amp; Music, Music, R&amp;B &amp; Soul]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[cat00000, abcat0600000, cat02001, cat02011]</td>\n",
       "      <td>[COMPACT DISC]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[430457]</td>\n",
       "      <td>[1989-01-06]</td>\n",
       "      <td>[R&amp;B]</td>\n",
       "      <td>[1007]</td>\n",
       "      <td>[Music]</td>\n",
       "      <td>[http://www.bestbuy.com/site/Greatest+Hits+%5B...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>[false]</td>\n",
       "      <td>[Penn,Michael]</td>\n",
       "      <td>[320474]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[cat02010]</td>\n",
       "      <td>[Best Buy, Movies &amp; Music, Music, Rock]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[cat00000, abcat0600000, cat02001, cat02010]</td>\n",
       "      <td>[COMPACT DISC]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[430466]</td>\n",
       "      <td>[1989-05-31]</td>\n",
       "      <td>[ROCK]</td>\n",
       "      <td>[1001]</td>\n",
       "      <td>[Music]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>[false]</td>\n",
       "      <td>[Parker,Graham]</td>\n",
       "      <td>[320476]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[cat02010]</td>\n",
       "      <td>[Best Buy, Movies &amp; Music, Music, Rock]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[cat00000, abcat0600000, cat02001, cat02010]</td>\n",
       "      <td>[COMPACT DISC]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[430475]</td>\n",
       "      <td>[1990-11-07]</td>\n",
       "      <td>[R&amp;B]</td>\n",
       "      <td>[1007]</td>\n",
       "      <td>[Music]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  accessories   active             artistName bestBuyItemId bestSellingRank  \\\n",
       "0          []  [false]         [Redbone,Leon]      [425502]              []   \n",
       "1          []  [false]            [Shadowfax]      [184387]              []   \n",
       "2          []   [true]  [The Pointer Sisters]      [321638]              []   \n",
       "3          []  [false]         [Penn,Michael]      [320474]              []   \n",
       "4          []  [false]        [Parker,Graham]      [320476]              []   \n",
       "\n",
       "  categoryLeaf                                   categoryPath  \\\n",
       "0   [cat02005]        [Best Buy, Movies & Music, Music, Folk]   \n",
       "1   [cat02008]     [Best Buy, Movies & Music, Music, New Age]   \n",
       "2   [cat02011]  [Best Buy, Movies & Music, Music, R&B & Soul]   \n",
       "3   [cat02010]        [Best Buy, Movies & Music, Music, Rock]   \n",
       "4   [cat02010]        [Best Buy, Movies & Music, Music, Rock]   \n",
       "\n",
       "   categoryPathCount                               categoryPathIds  \\\n",
       "0                4.0  [cat00000, abcat0600000, cat02001, cat02005]   \n",
       "1                4.0  [cat00000, abcat0600000, cat02001, cat02008]   \n",
       "2                4.0  [cat00000, abcat0600000, cat02001, cat02011]   \n",
       "3                4.0  [cat00000, abcat0600000, cat02001, cat02010]   \n",
       "4                4.0  [cat00000, abcat0600000, cat02001, cat02010]   \n",
       "\n",
       "            class  ... shortDescription shortDescriptionHtml       sku  \\\n",
       "0  [COMPACT DISC]  ...               []                   []  [430439]   \n",
       "1  [COMPACT DISC]  ...               []                   []  [430448]   \n",
       "2  [COMPACT DISC]  ...               []                   []  [430457]   \n",
       "3  [COMPACT DISC]  ...               []                   []  [430466]   \n",
       "4  [COMPACT DISC]  ...               []                   []  [430475]   \n",
       "\n",
       "      startDate             subclass subclassId     type  \\\n",
       "0  [1989-11-24]               [ROCK]     [1001]  [Music]   \n",
       "1  [1990-07-10]  [JAZZ-CONTEMPORARY]     [1002]  [Music]   \n",
       "2  [1989-01-06]                [R&B]     [1007]  [Music]   \n",
       "3  [1989-05-31]               [ROCK]     [1001]  [Music]   \n",
       "4  [1990-11-07]                [R&B]     [1007]  [Music]   \n",
       "\n",
       "                                                 url weight width  \n",
       "0                                                 []     []    []  \n",
       "1                                                 []     []    []  \n",
       "2  [http://www.bestbuy.com/site/Greatest+Hits+%5B...     []    []  \n",
       "3                                                 []     []    []  \n",
       "4                                                 []     []    []  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from lxml import etree\n",
    "def extract_records(file: str, mappings: dict) -> pd.DataFrame:\n",
    "    \"\"\"Extract details from XML file\n",
    "  \n",
    "    Args:\n",
    "        file (str): Path to the XML file containing details.\n",
    "        mappings (dict): A dictionary of mappings to extract\n",
    "  \n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas dataframe with records\n",
    "    \"\"\"\n",
    "    nodes = etree.parse(file).getroot().findall(\"./product\") \n",
    "    records = pd.DataFrame([\n",
    "      {k: node.xpath(v) for k, v in mappings.items()} \n",
    "      for node in nodes\n",
    "      if len(node.xpath(\"productId/text()\")) > 0\n",
    "    ])\n",
    "    return records\n",
    "\n",
    "records = extract_records(FILES[1], mappings)\n",
    "records.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now write a function to batch the records and save each batch as a parquet file. We batch the records so that we can index the batches in parallel, while keeping the batch size manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/tmp/products_0001_2570_to_430420-2.parquet',\n",
       " '/tmp/products_0001_2570_to_430420-1.parquet',\n",
       " '/tmp/products_0001_2570_to_430420-0.parquet']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "def save_records(records: pd.DataFrame, file: str, batch_size: int = 2000) -> int:\n",
    "    \"\"\"Save product records in batches to pickle files.\n",
    "\n",
    "    Args:\n",
    "        records (pd.DataFrame): Records to save.\n",
    "        file (str): The path to save the file.\n",
    "        batch_size (int, optional): The number of records in a batch. Defaults to 2000.\n",
    "    Returns:\n",
    "        int: The number of records extracted and saved.\n",
    "    \"\"\"\n",
    "    for idx, start in enumerate(range(0, len(records), batch_size)):\n",
    "        batch = records.iloc[start : start + batch_size]\n",
    "        batch.to_parquet(f\"{file}-{idx}.parquet\")\n",
    "    return len(records)\n",
    "\n",
    "save_records(records, f\"/tmp/{Path(FILES[0]).stem}\", batch_size = 2000)\n",
    "glob.glob(\"/tmp/*.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and Save Records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compose the two functions we wrote earlier to extact and save product records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/tmp/test-1/products_0001_2570_to_430420-2.parquet',\n",
       " '/tmp/test-1/products_0001_2570_to_430420-1.parquet',\n",
       " '/tmp/test-1/products_0001_2570_to_430420-0.parquet']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_and_save_records(file: str, output_dir: str, mappings: dict, batch_size:int = 2000) -> int:\n",
    "    \"\"\"Extract product records from XML files and save them in batches to pickle files.\n",
    "\n",
    "    Args:\n",
    "        file (str): XML file to extract records from.\n",
    "        output_dir (str):  The directory to save the pickle files.\n",
    "        mappings (dict): A dictionary of mappings to extract.\n",
    "        batch_size (int, optional): The maximum number of records in a batch. Defaults to 2000.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of records extracted and saved.\n",
    "    \"\"\"\n",
    "    records = extract_records(file, mappings)\n",
    "    output_file = Path(output_dir) / Path(file).stem\n",
    "    save_records(records, output_file, batch_size)\n",
    "    return len(records)\n",
    "\n",
    "Path(\"/tmp/test-1\").mkdir(parents=True, exist_ok=True)\n",
    "extract_and_save_records(FILES[0], \"/tmp/test-1\", mappings, 2000)\n",
    "glob.glob(\"/tmp/test-1/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and Save All Records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now write a function that loops through all the XML files and calls the `extract_and_save_records` function on each of them. We can use `ProcessPoolExecutor()` from `concurrent.futures` to parallelize the ingesion pipeline and speed it up significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gitpod/.pyenv/versions/3.9.7/envs/search_fundamentals/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 256/256 [10:06<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1275077 records from 256 files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from itertools import repeat\n",
    "from functools import partial\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "def extract_and_save_records_all(source_dir: str, output_dir: str, mappings: dict, batch_size:int = 2000):\n",
    "    \"\"\"Extract product records from XML files and save them in batches\n",
    "\n",
    "    Args:\n",
    "        source_dir (str): _description_\n",
    "        output_dir (str): _description_\n",
    "        mappings (dict): _description_\n",
    "        batch_size (int, optional): _description_. Defaults to 2000.\n",
    "    \"\"\"\n",
    "    os.mkdir(output_dir)\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    files = glob.glob(source_dir + \"/*.xml\")\n",
    "    extract_and_save_records_from_file = partial(\n",
    "        extract_and_save_records, \n",
    "        mappings = mappings, \n",
    "        output_dir = output_dir, \n",
    "        batch_size = batch_size\n",
    "    )\n",
    "    # with ProcessPoolExecutor(max_workers=8) as pool:\n",
    "    #     records = list(tqdm(pool.map(extract_and_save_records_from_file, files), total=len(files)))\n",
    "    records = process_map(extract_and_save_records_from_file, files, max_workers = 8)\n",
    "    print(f\"Extracted {sum(records)} records from {len(files)} files\")\n",
    "\n",
    "OUTPUT_DIR = \"/workspace/datasets/products\"\n",
    "extract_and_save_records_all(SOURCE_DIR, OUTPUT_DIR, mappings, batch_size = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"fc81333c71df\",\n",
      "  \"cluster_name\": \"docker-cluster\",\n",
      "  \"cluster_uuid\": \"_Xsvjrs0TJOF3p6QlsWnJA\",\n",
      "  \"version\": {\n",
      "    \"distribution\": \"opensearch\",\n",
      "    \"number\": \"2.9.0\",\n",
      "    \"build_type\": \"tar\",\n",
      "    \"build_hash\": \"1164221ee2b8ba3560f0ff492309867beea28433\",\n",
      "    \"build_date\": \"2023-07-18T21:23:29.367080729Z\",\n",
      "    \"build_snapshot\": false,\n",
      "    \"lucene_version\": \"9.7.0\",\n",
      "    \"minimum_wire_compatibility_version\": \"7.10.0\",\n",
      "    \"minimum_index_compatibility_version\": \"7.0.0\"\n",
      "  },\n",
      "  \"tagline\": \"The OpenSearch Project: https://opensearch.org/\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from opensearchpy import OpenSearch\n",
    "from IPython.display import JSON\n",
    "import json\n",
    "\n",
    "def print_json(x):\n",
    "    print(json.dumps(x, indent = 2))\n",
    "    \n",
    "client = OpenSearch(\n",
    "    hosts = [{\"host\": \"localhost\", \"port\": 9200}],\n",
    "    http_auth = (\"admin\", \"admin\"),\n",
    "    use_ssl = True,\n",
    "    verify_certs = False,\n",
    "    ssl_assert_hostname = False,\n",
    "    ssl_show_warn = False,\n",
    ")\n",
    "print_json(client.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_PRODUCT = 'bbuy_products'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.indices.delete(INDEX_PRODUCT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(\"../week1/mappings_product.yml\", \"r\") as w:\n",
    "  mappings_product = yaml.safe_load(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"acknowledged\": true,\n",
      "  \"shards_acknowledged\": true,\n",
      "  \"index\": \"bbuy_products\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "body = yaml.safe_load(\"\"\"\n",
    "settings:\n",
    "  index:\n",
    "    query:\n",
    "      default_field: body\n",
    "\"\"\")\n",
    "body['mappings'] = mappings_product[INDEX_PRODUCT]['mappings']\n",
    "response = client.indices.create(INDEX_PRODUCT, body = body)\n",
    "print_json(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mappings = client.indices.get_mapping(INDEX_PRODUCT)\n",
    "# with open(\"../week1/mappings_product.yml\", \"w\") as w:\n",
    "#     yaml.safe_dump(mappings, w, indent = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import pandas as pd\n",
    "from opensearchpy.helpers import bulk\n",
    "from functools import partial\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "from itertools import repeat\n",
    "\n",
    "def index_batch(batch: pd.DataFrame, index_name:str):\n",
    "    records = batch.to_dict(orient = \"records\")\n",
    "    docs = [{\"_id\": record[\"sku\"][0], \"_index\": index_name, \"_source\": record, } for record in records]\n",
    "    bulk(client, docs, request_timeout=60)\n",
    "    return len(batch)\n",
    "\n",
    "\n",
    "def index_batches(batches, index_name:str):\n",
    "    index_batch_partial = partial(index_batch, index_name = index_name)\n",
    "    results = process_map(index_batch_partial, batches, max_workers=8)\n",
    "    print(f\"Indexed {sum(results)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/workspace/datasets/products/products_0001_2570_to_430420-0.parquet',\n",
       " '/workspace/datasets/products/products_0001_2570_to_430420-1.parquet',\n",
       " '/workspace/datasets/products/products_0001_2570_to_430420-2.parquet',\n",
       " '/workspace/datasets/products/products_0002_430439_to_518210-0.parquet',\n",
       " '/workspace/datasets/products/products_0002_430439_to_518210-1.parquet']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "OUTPUT_DIR = \"/workspace/datasets/products\"\n",
    "output_files = sorted(glob.glob(f\"{OUTPUT_DIR}/*.parquet\"))[:5]\n",
    "output_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 9000 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "index_batches(batches, INDEX_PRODUCT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'epoch': '1696891196', 'timestamp': '22:39:56', 'count': '9000'}]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.indices.refresh(index = INDEX_PRODUCT)\n",
    "client.cat.count(index = INDEX_PRODUCT, format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_url = \"https://3000-ramnathv-searchfundamen-7vo4f3nho9y.ws-us105.gitpod.io/search/query?&query=*&filter.name=regularPrice&regularPrice.type=range&regularPrice.displayName=Price&regularPrice.from=30.01&regularPrice.to=40.0&filter.name=department.keyword&department.keyword.type=terms&department.keyword.key=VIDEO/COMPACT%20DISC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': ['*'],\n",
       " 'filter.name': ['regularPrice', 'department.keyword'],\n",
       " 'regularPrice.type': ['range'],\n",
       " 'regularPrice.displayName': ['Price'],\n",
       " 'regularPrice.from': ['30.01'],\n",
       " 'regularPrice.to': ['40.0'],\n",
       " 'department.keyword.type': ['terms'],\n",
       " 'department.keyword.key': ['VIDEO/COMPACT DISC']}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.parse import urlparse, parse_qs\n",
    "parsed_url = urlparse(query_url)\n",
    "query_params = parse_qs(parsed_url.query)\n",
    "query_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'&query=*&filter.name=regularPrice&regularPrice.type=range&regularPrice.displayName=Price&regularPrice.from=30.01&regularPrice.to=40.0&filter.name=department.keyword&department.keyword.type=terms&department.keyword.key=VIDEO/COMPACT%20DISC'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_url.query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'range': {'regularPrice': {'gte': '30.01', 'lt': '40.0'}}}, {'term': {'department': 'terms'}}]\n"
     ]
    }
   ],
   "source": [
    "filters = []\n",
    "for _filter in query_params['filter.name']:\n",
    "    if _filter == 'regularPrice':\n",
    "        range = dict(regularPrice = dict(\n",
    "            gte = query_params['regularPrice.from'][0],\n",
    "            lt = query_params['regularPrice.to'][0]\n",
    "        ))\n",
    "        filters.append(dict(range = range))\n",
    "    elif _filter == \"department.keyword\":\n",
    "        term = dict(\n",
    "            department = query_params[\"department.keyword.type\"][0]\n",
    "        )\n",
    "        filters.append(dict(term = term))\n",
    "print(filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters: [\n",
    "    {'range': {'regularPrice': {'gte': '30.01', 'lt': '40.0'}}}, \n",
    "    {'term': {'department': 'VIDEO/COMPACT DISC'}}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters Input\n",
    "['department.keyword']\n",
    "Filters: [{'term': {'department': 'PHOTO/COMMODITIES'}}]\n",
    "Display Filters\n",
    "['department.keyword: PHOTO/COMMODITIES']\n",
    "Applied Filters\n",
    "&filter.name=department.keyword&department.keyword.type=terms&department.keyword.displayName=department.keyword&department.keyword.fieldName=department&department.keyword.key=PHOTO/COMMODITIES"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search_fundamentals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
