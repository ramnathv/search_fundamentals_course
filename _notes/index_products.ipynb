{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index Product Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by extracting product data from the XML files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of XML files: 256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/workspace/datasets/product_data/products/products_0001_2570_to_430420.xml',\n",
       " '/workspace/datasets/product_data/products/products_0002_430439_to_518210.xml',\n",
       " '/workspace/datasets/product_data/products/products_0003_518229_to_606384.xml',\n",
       " '/workspace/datasets/product_data/products/products_0004_606428_to_722720.xml',\n",
       " '/workspace/datasets/product_data/products/products_0005_722800_to_846222.xml']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "SOURCE_DIR = \"/workspace/datasets/product_data/products\"\n",
    "FILES = glob.glob(f\"{SOURCE_DIR}/*.xml\")\n",
    "print(f\"Number of XML files: {len(FILES)}\")\n",
    "FILES[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us read `mappings.yaml` to get the xpath selectors associated with each field to extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['accessories', 'active', 'artistName', 'bestBuyItemId', 'bestSellingRank', 'categoryLeaf', 'categoryPath', 'categoryPathCount', 'categoryPathIds', 'class', 'classId', 'color', 'condition', 'crossSell', 'customerReviewAverage', 'customerReviewCount', 'department', 'departmentId', 'depth', 'description', 'digital', 'features', 'frequentlyPurchasedWith', 'height', 'homeDelivery', 'image', 'inStoreAvailability', 'inStorePickup', 'longDescription', 'longDescriptionHtml', 'manufacturer', 'modelNumber', 'name', 'onSale', 'onlineAvailability', 'productId', 'quantityLimit', 'regularPrice', 'relatedProducts', 'releaseDate', 'salePrice', 'salesRankLongTerm', 'salesRankMediumTerm', 'salesRankShortTerm', 'shippingCost', 'shippingWeight', 'shortDescription', 'shortDescriptionHtml', 'sku', 'startDate', 'subclass', 'subclassId', 'type', 'url', 'weight', 'width'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "with open(\"/workspace/datasets/mappings.yaml\", \"r\") as handle:\n",
    "    mappings = yaml.safe_load(handle)\n",
    "mappings.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us write a function to extract product records from a XML file and return a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accessories</th>\n",
       "      <th>active</th>\n",
       "      <th>artistName</th>\n",
       "      <th>bestBuyItemId</th>\n",
       "      <th>bestSellingRank</th>\n",
       "      <th>categoryLeaf</th>\n",
       "      <th>categoryPath</th>\n",
       "      <th>categoryPathCount</th>\n",
       "      <th>categoryPathIds</th>\n",
       "      <th>class</th>\n",
       "      <th>...</th>\n",
       "      <th>shortDescription</th>\n",
       "      <th>shortDescriptionHtml</th>\n",
       "      <th>sku</th>\n",
       "      <th>startDate</th>\n",
       "      <th>subclass</th>\n",
       "      <th>subclassId</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>weight</th>\n",
       "      <th>width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>[false]</td>\n",
       "      <td>[Redbone,Leon]</td>\n",
       "      <td>[425502]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[cat02005]</td>\n",
       "      <td>[Best Buy, Movies &amp; Music, Music, Folk]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[cat00000, abcat0600000, cat02001, cat02005]</td>\n",
       "      <td>[COMPACT DISC]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[430439]</td>\n",
       "      <td>[1989-11-24]</td>\n",
       "      <td>[ROCK]</td>\n",
       "      <td>[1001]</td>\n",
       "      <td>[Music]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>[false]</td>\n",
       "      <td>[Shadowfax]</td>\n",
       "      <td>[184387]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[cat02008]</td>\n",
       "      <td>[Best Buy, Movies &amp; Music, Music, New Age]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[cat00000, abcat0600000, cat02001, cat02008]</td>\n",
       "      <td>[COMPACT DISC]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[430448]</td>\n",
       "      <td>[1990-07-10]</td>\n",
       "      <td>[JAZZ-CONTEMPORARY]</td>\n",
       "      <td>[1002]</td>\n",
       "      <td>[Music]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>[true]</td>\n",
       "      <td>[The Pointer Sisters]</td>\n",
       "      <td>[321638]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[cat02011]</td>\n",
       "      <td>[Best Buy, Movies &amp; Music, Music, R&amp;B &amp; Soul]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[cat00000, abcat0600000, cat02001, cat02011]</td>\n",
       "      <td>[COMPACT DISC]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[430457]</td>\n",
       "      <td>[1989-01-06]</td>\n",
       "      <td>[R&amp;B]</td>\n",
       "      <td>[1007]</td>\n",
       "      <td>[Music]</td>\n",
       "      <td>[http://www.bestbuy.com/site/Greatest+Hits+%5B...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>[false]</td>\n",
       "      <td>[Penn,Michael]</td>\n",
       "      <td>[320474]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[cat02010]</td>\n",
       "      <td>[Best Buy, Movies &amp; Music, Music, Rock]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[cat00000, abcat0600000, cat02001, cat02010]</td>\n",
       "      <td>[COMPACT DISC]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[430466]</td>\n",
       "      <td>[1989-05-31]</td>\n",
       "      <td>[ROCK]</td>\n",
       "      <td>[1001]</td>\n",
       "      <td>[Music]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>[false]</td>\n",
       "      <td>[Parker,Graham]</td>\n",
       "      <td>[320476]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[cat02010]</td>\n",
       "      <td>[Best Buy, Movies &amp; Music, Music, Rock]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[cat00000, abcat0600000, cat02001, cat02010]</td>\n",
       "      <td>[COMPACT DISC]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[430475]</td>\n",
       "      <td>[1990-11-07]</td>\n",
       "      <td>[R&amp;B]</td>\n",
       "      <td>[1007]</td>\n",
       "      <td>[Music]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  accessories   active             artistName bestBuyItemId bestSellingRank  \\\n",
       "0          []  [false]         [Redbone,Leon]      [425502]              []   \n",
       "1          []  [false]            [Shadowfax]      [184387]              []   \n",
       "2          []   [true]  [The Pointer Sisters]      [321638]              []   \n",
       "3          []  [false]         [Penn,Michael]      [320474]              []   \n",
       "4          []  [false]        [Parker,Graham]      [320476]              []   \n",
       "\n",
       "  categoryLeaf                                   categoryPath  \\\n",
       "0   [cat02005]        [Best Buy, Movies & Music, Music, Folk]   \n",
       "1   [cat02008]     [Best Buy, Movies & Music, Music, New Age]   \n",
       "2   [cat02011]  [Best Buy, Movies & Music, Music, R&B & Soul]   \n",
       "3   [cat02010]        [Best Buy, Movies & Music, Music, Rock]   \n",
       "4   [cat02010]        [Best Buy, Movies & Music, Music, Rock]   \n",
       "\n",
       "   categoryPathCount                               categoryPathIds  \\\n",
       "0                4.0  [cat00000, abcat0600000, cat02001, cat02005]   \n",
       "1                4.0  [cat00000, abcat0600000, cat02001, cat02008]   \n",
       "2                4.0  [cat00000, abcat0600000, cat02001, cat02011]   \n",
       "3                4.0  [cat00000, abcat0600000, cat02001, cat02010]   \n",
       "4                4.0  [cat00000, abcat0600000, cat02001, cat02010]   \n",
       "\n",
       "            class  ... shortDescription shortDescriptionHtml       sku  \\\n",
       "0  [COMPACT DISC]  ...               []                   []  [430439]   \n",
       "1  [COMPACT DISC]  ...               []                   []  [430448]   \n",
       "2  [COMPACT DISC]  ...               []                   []  [430457]   \n",
       "3  [COMPACT DISC]  ...               []                   []  [430466]   \n",
       "4  [COMPACT DISC]  ...               []                   []  [430475]   \n",
       "\n",
       "      startDate             subclass subclassId     type  \\\n",
       "0  [1989-11-24]               [ROCK]     [1001]  [Music]   \n",
       "1  [1990-07-10]  [JAZZ-CONTEMPORARY]     [1002]  [Music]   \n",
       "2  [1989-01-06]                [R&B]     [1007]  [Music]   \n",
       "3  [1989-05-31]               [ROCK]     [1001]  [Music]   \n",
       "4  [1990-11-07]                [R&B]     [1007]  [Music]   \n",
       "\n",
       "                                                 url weight width  \n",
       "0                                                 []     []    []  \n",
       "1                                                 []     []    []  \n",
       "2  [http://www.bestbuy.com/site/Greatest+Hits+%5B...     []    []  \n",
       "3                                                 []     []    []  \n",
       "4                                                 []     []    []  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from lxml import etree\n",
    "def extract_records(file: str, mappings: dict) -> pd.DataFrame:\n",
    "    \"\"\"Extract details from XML file\n",
    "  \n",
    "    Args:\n",
    "        file (str): Path to the XML file containing details.\n",
    "        mappings (dict): A dictionary of mappings to extract\n",
    "  \n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas dataframe with records\n",
    "    \"\"\"\n",
    "    nodes = etree.parse(file).getroot().findall(\"./product\") \n",
    "    records = pd.DataFrame([\n",
    "      {k: node.xpath(v) for k, v in mappings.items()} \n",
    "      for node in nodes\n",
    "      if len(node.xpath(\"productId/text()\")) > 0\n",
    "    ])\n",
    "    return records\n",
    "\n",
    "records = extract_records(FILES[1], mappings)\n",
    "records.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now write a function to batch the records and save each batch as a parquet file. We batch the records so that we can index the batches in parallel, while keeping the batch size manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/tmp/products_0001_2570_to_430420-2.parquet',\n",
       " '/tmp/products_0001_2570_to_430420-1.parquet',\n",
       " '/tmp/products_0001_2570_to_430420-0.parquet']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "def save_records(records: pd.DataFrame, file: str, batch_size: int = 2000) -> int:\n",
    "    \"\"\"Save product records in batches to pickle files.\n",
    "\n",
    "    Args:\n",
    "        records (pd.DataFrame): Records to save.\n",
    "        file (str): The path to save the file.\n",
    "        batch_size (int, optional): The number of records in a batch. Defaults to 2000.\n",
    "    Returns:\n",
    "        int: The number of records extracted and saved.\n",
    "    \"\"\"\n",
    "    for idx, start in enumerate(range(0, len(records), batch_size)):\n",
    "        batch = records.iloc[start : start + batch_size]\n",
    "        batch.to_parquet(f\"{file}-{idx}.parquet\")\n",
    "    return len(records)\n",
    "\n",
    "save_records(records, f\"/tmp/{Path(FILES[0]).stem}\", batch_size = 2000)\n",
    "glob.glob(\"/tmp/*.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and Save Records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compose the two functions we wrote earlier to extact and save product records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/tmp/test-1/products_0001_2570_to_430420-2.parquet',\n",
       " '/tmp/test-1/products_0001_2570_to_430420-1.parquet',\n",
       " '/tmp/test-1/products_0001_2570_to_430420-0.parquet']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_and_save_records(file: str, output_dir: str, mappings: dict, batch_size:int = 2000) -> int:\n",
    "    \"\"\"Extract product records from XML files and save them in batches to pickle files.\n",
    "\n",
    "    Args:\n",
    "        file (str): XML file to extract records from.\n",
    "        output_dir (str):  The directory to save the pickle files.\n",
    "        mappings (dict): A dictionary of mappings to extract.\n",
    "        batch_size (int, optional): The maximum number of records in a batch. Defaults to 2000.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of records extracted and saved.\n",
    "    \"\"\"\n",
    "    records = extract_records(file, mappings)\n",
    "    output_file = Path(output_dir) / Path(file).stem\n",
    "    save_records(records, output_file, batch_size)\n",
    "    return len(records)\n",
    "\n",
    "Path(\"/tmp/test-1\").mkdir(parents=True, exist_ok=True)\n",
    "extract_and_save_records(FILES[0], \"/tmp/test-1\", mappings, 2000)\n",
    "glob.glob(\"/tmp/test-1/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and Save All Records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now write a function that loops through all the XML files and calls the `extract_and_save_records` function on each of them. We can use `ProcessPoolExecutor()` from `concurrent.futures` to parallelize the ingesion pipeline and speed it up significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [07:50<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1275077 records from 256 files\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from itertools import repeat\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "def extract_and_save_records_all(source_dir: str, output_dir: str, mappings: dict, batch_size:int = 2000):\n",
    "    \"\"\"Extract product records from XML files and save them in batches\n",
    "\n",
    "    Args:\n",
    "        source_dir (str): _description_\n",
    "        output_dir (str): _description_\n",
    "        mappings (dict): _description_\n",
    "        batch_size (int, optional): _description_. Defaults to 2000.\n",
    "    \"\"\"\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    files = glob.glob(source_dir + \"/*.xml\")\n",
    "    extract_and_save_records_from_file = partial(\n",
    "        extract_and_save_records, \n",
    "        mappings = mappings, \n",
    "        output_dir = output_dir, \n",
    "        batch_size = batch_size\n",
    "    )\n",
    "    with ProcessPoolExecutor(max_workers=8) as pool:\n",
    "        records = list(tqdm(pool.map(extract_and_save_records_from_file, files), total=len(files)))\n",
    "    print(f\"Extracted {sum(records)} records from {len(files)} files\")\n",
    "\n",
    "OUTPUT_DIR = \"/workspace/datasets/products\"\n",
    "extract_and_save_records_all(SOURCE_DIR, OUTPUT_DIR, mappings, batch_size = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_files = sorted(glob.glob(f\"{OUTPUT_DIR}/*.parquet\"))\n",
    "batches = (pd.read_parquet(file) for file in output_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search_fundamentals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
